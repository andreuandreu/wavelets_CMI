{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Desktop/Dropbox/transfer_inormation_prague/code/Entropies_tests/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"./\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/project_TE/package_CMI_prague/Project.toml`\n",
      " \u001b[90m [5732040d] \u001b[39m\u001b[37mDelayEmbeddings v1.20.0\u001b[39m\n",
      " \u001b[90m [634d3b9d] \u001b[39m\u001b[37mDrWatson v1.16.6\u001b[39m\n",
      " \u001b[90m [ed8fcbec] \u001b[39m\u001b[37mEntropies v0.1.0\u001b[39m\n",
      " \u001b[90m [fd094767] \u001b[39m\u001b[37mSuppressor v0.2.0\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/Desktop/Dropbox/transfer_inormation_prague/code/Entropies_tests/Project.toml`\n",
      " \u001b[90m [5732040d] \u001b[39m\u001b[92m+ DelayEmbeddings v1.19.3\u001b[39m\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/Desktop/Dropbox/transfer_inormation_prague/code/Entropies_tests/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"DelayEmbeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m AbstractFFTs ─ v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OffsetArrays ─ v1.5.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Adapt ──────── v2.4.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FFTW ───────── v1.3.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Entropies ──── v0.11.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Polynomials ── v1.2.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DSP ────────── v0.6.10\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TimeZones ──── v1.5.3\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/Desktop/Dropbox/transfer_inormation_prague/code/Entropies_tests/Project.toml`\n",
      " \u001b[90m [ed8fcbec] \u001b[39m\u001b[92m+ Entropies v0.11.0\u001b[39m\n",
      "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/Desktop/Dropbox/transfer_inormation_prague/code/Entropies_tests/Manifest.toml`\n",
      " \u001b[90m [621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.0.0\u001b[39m\n",
      " \u001b[90m [79e6a3ab] \u001b[39m\u001b[92m+ Adapt v2.4.0\u001b[39m\n",
      " \u001b[90m [56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.3.0\u001b[39m\n",
      " \u001b[90m [34da2185] \u001b[39m\u001b[92m+ Compat v3.25.0\u001b[39m\n",
      " \u001b[90m [e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v0.3.4+0\u001b[39m\n",
      " \u001b[90m [717857b8] \u001b[39m\u001b[92m+ DSP v0.6.10\u001b[39m\n",
      " \u001b[90m [9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.4.0\u001b[39m\n",
      " \u001b[90m [864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.18.8\u001b[39m\n",
      " \u001b[90m [5732040d] \u001b[39m\u001b[92m+ DelayEmbeddings v1.19.3\u001b[39m\n",
      " \u001b[90m [b4f34e82] \u001b[39m\u001b[92m+ Distances v0.10.0\u001b[39m\n",
      " \u001b[90m [31c24e10] \u001b[39m\u001b[92m+ Distributions v0.24.10\u001b[39m\n",
      " \u001b[90m [ed8fcbec] \u001b[39m\u001b[92m+ Entropies v0.11.0\u001b[39m\n",
      " \u001b[90m [e2ba6199] \u001b[39m\u001b[92m+ ExprTools v0.1.3\u001b[39m\n",
      " \u001b[90m [8f5d6c58] \u001b[39m\u001b[92m+ EzXML v1.1.0\u001b[39m\n",
      " \u001b[90m [7a1cc6ca] \u001b[39m\u001b[92m+ FFTW v1.3.1\u001b[39m\n",
      " \u001b[90m [f5851436] \u001b[39m\u001b[92m+ FFTW_jll v3.3.9+7\u001b[39m\n",
      " \u001b[90m [1a297f60] \u001b[39m\u001b[92m+ FillArrays v0.10.2\u001b[39m\n",
      " \u001b[90m [1d5cc7b8] \u001b[39m\u001b[92m+ IntelOpenMP_jll v2018.0.3+0\u001b[39m\n",
      " \u001b[90m [d8418881] \u001b[39m\u001b[92m+ Intervals v1.5.0\u001b[39m\n",
      " \u001b[90m [c8e1da08] \u001b[39m\u001b[92m+ IterTools v1.3.0\u001b[39m\n",
      " \u001b[90m [692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.1.4\u001b[39m\n",
      " \u001b[90m [94ce4f54] \u001b[39m\u001b[92m+ Libiconv_jll v1.16.0+7\u001b[39m\n",
      " \u001b[90m [856f044c] \u001b[39m\u001b[92m+ MKL_jll v2020.2.254+0\u001b[39m\n",
      " \u001b[90m [e1d29d7a] \u001b[39m\u001b[92m+ Missings v0.4.4\u001b[39m\n",
      " \u001b[90m [78c3b35d] \u001b[39m\u001b[92m+ Mocking v0.7.1\u001b[39m\n",
      " \u001b[90m [b8a86587] \u001b[39m\u001b[92m+ NearestNeighbors v0.4.8\u001b[39m\n",
      " \u001b[90m [645ca80c] \u001b[39m\u001b[92m+ Neighborhood v0.2.2\u001b[39m\n",
      " \u001b[90m [6fe1bfb0] \u001b[39m\u001b[92m+ OffsetArrays v1.5.0\u001b[39m\n",
      " \u001b[90m [efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.3+4\u001b[39m\n",
      " \u001b[90m [bac558e1] \u001b[39m\u001b[92m+ OrderedCollections v1.3.2\u001b[39m\n",
      " \u001b[90m [90014a1f] \u001b[39m\u001b[92m+ PDMats v0.10.1\u001b[39m\n",
      " \u001b[90m [f27b6e38] \u001b[39m\u001b[92m+ Polynomials v1.2.0\u001b[39m\n",
      " \u001b[90m [1fd47b50] \u001b[39m\u001b[92m+ QuadGK v2.4.1\u001b[39m\n",
      " \u001b[90m [3cdcf5f2] \u001b[39m\u001b[92m+ RecipesBase v1.1.1\u001b[39m\n",
      " \u001b[90m [189a3867] \u001b[39m\u001b[92m+ Reexport v0.2.0\u001b[39m\n",
      " \u001b[90m [79098fc4] \u001b[39m\u001b[92m+ Rmath v0.6.1\u001b[39m\n",
      " \u001b[90m [f50d1b31] \u001b[39m\u001b[92m+ Rmath_jll v0.2.2+1\u001b[39m\n",
      " \u001b[90m [a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v0.3.1\u001b[39m\n",
      " \u001b[90m [276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v0.10.3\u001b[39m\n",
      " \u001b[90m [90137ffa] \u001b[39m\u001b[92m+ StaticArrays v1.0.1\u001b[39m\n",
      " \u001b[90m [2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.33.2\u001b[39m\n",
      " \u001b[90m [4c63d2b9] \u001b[39m\u001b[92m+ StatsFuns v0.9.6\u001b[39m\n",
      " \u001b[90m [f269a46b] \u001b[39m\u001b[92m+ TimeZones v1.5.3\u001b[39m\n",
      " \u001b[90m [29a6e085] \u001b[39m\u001b[92m+ Wavelets v0.9.2\u001b[39m\n",
      " \u001b[90m [02c8fc9c] \u001b[39m\u001b[92m+ XML2_jll v2.9.10+3\u001b[39m\n",
      " \u001b[90m [83775a58] \u001b[39m\u001b[92m+ Zlib_jll v1.2.11+18\u001b[39m\n",
      " \u001b[90m [2a0f44e3] \u001b[39m\u001b[92m+ Base64\u001b[39m\n",
      " \u001b[90m [ade2ca70] \u001b[39m\u001b[92m+ Dates\u001b[39m\n",
      " \u001b[90m [8bb1440f] \u001b[39m\u001b[92m+ DelimitedFiles\u001b[39m\n",
      " \u001b[90m [8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
      " \u001b[90m [b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils\u001b[39m\n",
      " \u001b[90m [76f85450] \u001b[39m\u001b[92m+ LibGit2\u001b[39m\n",
      " \u001b[90m [8f399da3] \u001b[39m\u001b[92m+ Libdl\u001b[39m\n",
      " \u001b[90m [37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra\u001b[39m\n",
      " \u001b[90m [56ddb016] \u001b[39m\u001b[92m+ Logging\u001b[39m\n",
      " \u001b[90m [d6f4376e] \u001b[39m\u001b[92m+ Markdown\u001b[39m\n",
      " \u001b[90m [a63ad114] \u001b[39m\u001b[92m+ Mmap\u001b[39m\n",
      " \u001b[90m [44cfe95a] \u001b[39m\u001b[92m+ Pkg\u001b[39m\n",
      " \u001b[90m [de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
      " \u001b[90m [3fa0cd96] \u001b[39m\u001b[92m+ REPL\u001b[39m\n",
      " \u001b[90m [9a3f8284] \u001b[39m\u001b[92m+ Random\u001b[39m\n",
      " \u001b[90m [ea8e919c] \u001b[39m\u001b[92m+ SHA\u001b[39m\n",
      " \u001b[90m [9e88b42a] \u001b[39m\u001b[92m+ Serialization\u001b[39m\n",
      " \u001b[90m [1a1011a3] \u001b[39m\u001b[92m+ SharedArrays\u001b[39m\n",
      " \u001b[90m [6462fe0b] \u001b[39m\u001b[92m+ Sockets\u001b[39m\n",
      " \u001b[90m [2f01184e] \u001b[39m\u001b[92m+ SparseArrays\u001b[39m\n",
      " \u001b[90m [10745b16] \u001b[39m\u001b[92m+ Statistics\u001b[39m\n",
      " \u001b[90m [4607b0f0] \u001b[39m\u001b[92m+ SuiteSparse\u001b[39m\n",
      " \u001b[90m [8dfed614] \u001b[39m\u001b[92m+ Test\u001b[39m\n",
      " \u001b[90m [cf7118a7] \u001b[39m\u001b[92m+ UUIDs\u001b[39m\n",
      " \u001b[90m [4ec0a83e] \u001b[39m\u001b[92m+ Unicode\u001b[39m\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m FFTW ─────→ `~/.julia/packages/FFTW/eADNB/deps/build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m TimeZones → `~/.julia/packages/TimeZones/K98G0/deps/build.log`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Entropies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelayEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1my\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1my\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22mWeighted\u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\n",
       "SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\n",
       "SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\n",
       "\\end{verbatim}\n",
       "Symbolic, permutation-based probabilities/entropy estimators.\n",
       "\n",
       "Uses embedding dimension $m = 3$ with embedding lag $\\tau = 1$ by default. The minimum dimension $m$ is 2 (there are no sorting permutations of single-element state vectors).\n",
       "\n",
       "\\subsection{Repeated values during symbolization}\n",
       "In the original implementation of permutation entropy \\footnotemark[BandtPompe2002], equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution \\footnotemark[Zunino2017]. Here, we resolve this issue by letting the user provide a custom \"less-than\" function. The keyword \\texttt{lt} accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (\\texttt{lt = Entropies.isless\\_rand}). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.\n",
       "\n",
       "To get the behaviour described in Bandt and Pompe (2002), use \\texttt{lt = Base.isless}).\n",
       "\n",
       "\\subsection{Properties of original signal preserved}\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{\\texttt{SymbolicPermutation}}: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt \\& Pompe et al. (2002)\\footnotemark[BandtPompe2002] and   Berger et al. (2019) \\footnotemark[Berger2019].\n",
       "\n",
       "\n",
       "\\item \\textbf{\\texttt{SymbolicWeightedPermutation}}: Like \\texttt{SymbolicPermutation}, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)\\footnotemark[Fadlallah2013].\n",
       "\n",
       "\n",
       "\\item \\textbf{\\texttt{SymbolicAmplitudeAwarePermutation}}: Like \\texttt{SymbolicPermutation}, but also encodes   amplitude information by considering a weighted combination of \\emph{absolute amplitudes}   of state vectors, and \\emph{relative differences between elements} of state vectors. See   description below for explanation of the weighting parameter \\texttt{A}. This implementation   is based on Azami \\& Escudero (2016) \\footnotemark[Azami2016].\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Probability estimation}\n",
       "\\subsubsection{Univariate time series}\n",
       "To estimate probabilities or entropies from univariate time series, use the following methods:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{probabilities(x::AbstractVector, est::SymbolicProbabilityEstimator)}. Constructs state vectors   from \\texttt{x} using embedding lag \\texttt{τ} and embedding dimension \\texttt{m}, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.\n",
       "\n",
       "\n",
       "\\item \\texttt{genentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; α=1, base = 2)} computes   probabilities by calling \\texttt{probabilities(x::AbstractVector, est)},   then computer the order-\\texttt{α} generalized entropy to the given base.\n",
       "\n",
       "\\end{itemize}\n",
       "\\paragraph{Speeding up repeated computations}\n",
       "A pre-allocated integer symbol array \\texttt{s} can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.\n",
       "\n",
       "\\emph{Note: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that \\texttt{length(x) == length(s)} if \\texttt{x} is a Dataset, or \\texttt{length(s) == length(x) - (m-1)τ} if \\texttt{x} is a univariate signal that is to be embedded first}.\n",
       "\n",
       "Use the following signatures (only works for \\texttt{SymbolicPermutation}).\n",
       "\n",
       "\\begin{verbatim}\n",
       "probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities\n",
       "probabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities\n",
       "\\end{verbatim}\n",
       "\\subsubsection{Multivariate datasets}\n",
       "Although not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ≥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the $L$ existing state vectors $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x_L}\\}$.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)}. Compute ordinal patterns of the   state vectors of \\texttt{x} directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.\n",
       "\n",
       "\n",
       "\\item \\texttt{genentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator)}. Computes probabilities from   symbol frequencies using \\texttt{probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)},   then computes the order-\\texttt{α} generalized (permutation) entropy to the given base.\n",
       "\n",
       "\\end{itemize}\n",
       "\\emph{Caveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for \\texttt{Dataset}s are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.}\n",
       "\n",
       "\\subsection{Description}\n",
       "All symbolic estimators use the same underlying approach to estimating probabilities.\n",
       "\n",
       "\\subsubsection{Embedding, ordinal patterns and symbolization}\n",
       "Consider the $n$-element univariate time series $\\{x(t) = x_1, x_2, \\ldots, x_n\\}$. Let $\\mathbf{x_i}^{m, \\tau} = \\{x_j, x_{j+\\tau}, \\ldots, x_{j+(m-1)\\tau}\\}$ for $j = 1, 2, \\ldots n - (m-1)\\tau$ be the $i$-th state vector in a delay reconstruction with embedding dimension $m$ and reconstruction lag $\\tau$. There are then $N = n - (m-1)\\tau$ state vectors.\n",
       "\n",
       "For an $m$-dimensional vector, there are $m!$ possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a \\emph{motif}. Let $\\pi_i^{m, \\tau}$ denote the motif associated with the $m$-dimensional state vector $\\mathbf{x_i}^{m, \\tau}$, and let $R$ be the number of distinct motifs that can be constructed from the $N$ state vectors. Then there are at most $R$ motifs; $R = N$ precisely when all motifs are unique, and $R = 1$ when all motifs are the same.\n",
       "\n",
       "Each unique motif $\\pi_i^{m, \\tau}$ can be mapped to a unique integer symbol $0 \\leq s_i \\leq M!-1$. Let $S(\\pi) : \\mathbb{R}^m \\to \\mathbb{N}_0$ be the function that maps the motif $\\pi$ to its symbol $s$, and let $\\Pi$ denote the set of symbols $\\Pi = \\{ s_i \\}_{i\\in \\{ 1, \\ldots, R\\}}$.\n",
       "\n",
       "\\subsubsection{Probability computation}\n",
       "\\paragraph{\\texttt{SymbolicPermutation}}\n",
       "The probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from \\footnotemark[Fadlallah2013]),\n",
       "\n",
       "$$p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left(\\mathbf{x}_k^{m, \\tau} \\right) }{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi} \\left(\\mathbf{x}_k^{m, \\tau} \\right)} = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left(\\mathbf{x}_k^{m, \\tau} \\right) }{N},$$\n",
       "where the function $\\mathbf{1}_A(u)$ is the indicator function of a set $A$. That     is, $\\mathbf{1}_A(u) = 1$ if $u \\in A$, and $\\mathbf{1}_A(u) = 0$ otherwise.\n",
       "\n",
       "\\paragraph{\\texttt{SymbolicAmplitudeAwarePermutation}}\n",
       "Amplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.\n",
       "\n",
       "$$p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, a_k}{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\,a_k} = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, a_k}{\\sum_{k=1}^N a_k}.$$\n",
       "The weights encoding amplitude information about state vector $\\mathbf{x}_i = (x_1^i, x_2^i, \\ldots, x_m^i)$ are\n",
       "\n",
       "$$a_i = \\dfrac{A}{m} \\sum_{k=1}^m |x_k^i | + \\dfrac{1-A}{d-1} \\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,$$\n",
       "with $0 \\leq A \\leq 1$. When $A=0$ , only internal differences between the elements of $\\mathbf{x}_i$ are weighted. Only mean amplitude of the state vector elements are weighted when $A=1$. With, $0<A<1$, a combined weighting is used.\n",
       "\n",
       "\\paragraph{\\texttt{SymbolicWeightedPermutation}}\n",
       "Weighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:\n",
       "\n",
       "$$p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i}\n",
       "\\left( \\mathbf{x}_k^{m, \\tau} \\right)\n",
       "\\, w_k}{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi}\n",
       "\\left( \\mathbf{x}_k^{m, \\tau} \\right) \\,w_k} = \\dfrac{\\sum_{k=1}^N\n",
       "\\mathbf{1}_{u:S(u) = s_i}\n",
       "\\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, w_k}{\\sum_{k=1}^N w_k}.$$\n",
       "The weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical ($w_j = \\beta \\,\\,\\, \\forall \\,\\,\\, j \\leq N$ and $\\beta > 0)$. Weights are dictated by the variance of the state vectors.\n",
       "\n",
       "Let the aritmetic mean of state vector $\\mathbf{x}_i$ be denoted by\n",
       "\n",
       "$$\\mathbf{\\hat{x}}_j^{m, \\tau} = \\frac{1}{m} \\sum_{k=1}^m x_{j + (k+1)\\tau}.$$\n",
       "Weights are then computed as\n",
       "\n",
       "$$w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j+(k+1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.$$\n",
       "\\emph{Note: in equation 7, section III, of the original paper, the authors write}\n",
       "\n",
       "$$w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j-(k-1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.$$\n",
       "\\emph{But given the formula they give for the arithmetic mean, this is \\textbf{not} the variance of $\\mathbf{x}_i$, because the indices are mixed: $x_{j+(k-1)\\tau}$ in the weights formula, vs. $x_{j+(k+1)\\tau}$ in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for $\\mathbf{x}_i$}.\n",
       "\n",
       "\\subsubsection{Entropy computation}\n",
       "The generalized order-\\texttt{α} Renyi entropy\\footnotemark[Rényi1960] can be computed over the probability distribution of symbols as $H(m, \\tau, \\alpha) = \\dfrac{\\alpha}{1-\\alpha} \\log \\left( \\sum_{j=1}^R p_j^\\alpha \\right)$. Permutation entropy, as described in Bandt and Pompe (2002), is just the limiting case as $α \\to1$, that is $H(m, \\tau) = - \\sum_j^R p(\\pi_j^{m, \\tau}) \\ln p(\\pi_j^{m, \\tau})$.\n",
       "\n",
       "\\emph{Note: Do not confuse the order of the generalized entropy (\\texttt{α}) with the order \\texttt{m} of the permutation entropy (which controls the symbol size). Permutation entropy is usually estimated with \\texttt{α = 1}, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.}\n",
       "\n",
       "\\footnotetext[BandtPompe2002]{Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n",
       "\n",
       "}\n",
       "\\footnotetext[Berger2019]{Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n",
       "\n",
       "}\n",
       "\\footnotetext[Fadlallah2013]{Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n",
       "\n",
       "}\n",
       "\\footnotetext[Rényi1960]{A. Rényi, \\emph{Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability}, pp 547 (1960)\n",
       "\n",
       "}\n",
       "\\footnotetext[Azami2016]{Azami, H., \\& Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n",
       "\n",
       "}\n",
       "\\footnotetext[Fadlallah2013]{Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n",
       "\n",
       "}\n",
       "\\footnotetext[Zunino2017]{Zunino, L., Olivares, F., Scholkmann, F., \\& Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n",
       "\n",
       "}\n"
      ],
      "text/markdown": [
       "```\n",
       "SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\n",
       "SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\n",
       "SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\n",
       "```\n",
       "\n",
       "Symbolic, permutation-based probabilities/entropy estimators.\n",
       "\n",
       "Uses embedding dimension $m = 3$ with embedding lag $\\tau = 1$ by default. The minimum dimension $m$ is 2 (there are no sorting permutations of single-element state vectors).\n",
       "\n",
       "## Repeated values during symbolization\n",
       "\n",
       "In the original implementation of permutation entropy [^BandtPompe2002], equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [^Zunino2017]. Here, we resolve this issue by letting the user provide a custom \"less-than\" function. The keyword `lt` accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (`lt = Entropies.isless_rand`). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.\n",
       "\n",
       "To get the behaviour described in Bandt and Pompe (2002), use `lt = Base.isless`).\n",
       "\n",
       "## Properties of original signal preserved\n",
       "\n",
       "  * **`SymbolicPermutation`**: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt & Pompe et al. (2002)[^BandtPompe2002] and   Berger et al. (2019) [^Berger2019].\n",
       "  * **`SymbolicWeightedPermutation`**: Like `SymbolicPermutation`, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)[^Fadlallah2013].\n",
       "  * **`SymbolicAmplitudeAwarePermutation`**: Like `SymbolicPermutation`, but also encodes   amplitude information by considering a weighted combination of *absolute amplitudes*   of state vectors, and *relative differences between elements* of state vectors. See   description below for explanation of the weighting parameter `A`. This implementation   is based on Azami & Escudero (2016) [^Azami2016].\n",
       "\n",
       "## Probability estimation\n",
       "\n",
       "### Univariate time series\n",
       "\n",
       "To estimate probabilities or entropies from univariate time series, use the following methods:\n",
       "\n",
       "  * `probabilities(x::AbstractVector, est::SymbolicProbabilityEstimator)`. Constructs state vectors   from `x` using embedding lag `τ` and embedding dimension `m`, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.\n",
       "  * `genentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; α=1, base = 2)` computes   probabilities by calling `probabilities(x::AbstractVector, est)`,   then computer the order-`α` generalized entropy to the given base.\n",
       "\n",
       "#### Speeding up repeated computations\n",
       "\n",
       "A pre-allocated integer symbol array `s` can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.\n",
       "\n",
       "*Note: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that `length(x) == length(s)` if `x` is a Dataset, or `length(s) == length(x) - (m-1)τ` if `x` is a univariate signal that is to be embedded first*.\n",
       "\n",
       "Use the following signatures (only works for `SymbolicPermutation`).\n",
       "\n",
       "```julia\n",
       "probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities\n",
       "probabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities\n",
       "```\n",
       "\n",
       "### Multivariate datasets\n",
       "\n",
       "Although not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ≥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the $L$ existing state vectors $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x_L}\\}$.\n",
       "\n",
       "  * `probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)`. Compute ordinal patterns of the   state vectors of `x` directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.\n",
       "  * `genentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator)`. Computes probabilities from   symbol frequencies using `probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)`,   then computes the order-`α` generalized (permutation) entropy to the given base.\n",
       "\n",
       "*Caveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for `Dataset`s are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.*\n",
       "\n",
       "## Description\n",
       "\n",
       "All symbolic estimators use the same underlying approach to estimating probabilities.\n",
       "\n",
       "### Embedding, ordinal patterns and symbolization\n",
       "\n",
       "Consider the $n$-element univariate time series $\\{x(t) = x_1, x_2, \\ldots, x_n\\}$. Let $\\mathbf{x_i}^{m, \\tau} = \\{x_j, x_{j+\\tau}, \\ldots, x_{j+(m-1)\\tau}\\}$ for $j = 1, 2, \\ldots n - (m-1)\\tau$ be the $i$-th state vector in a delay reconstruction with embedding dimension $m$ and reconstruction lag $\\tau$. There are then $N = n - (m-1)\\tau$ state vectors.\n",
       "\n",
       "For an $m$-dimensional vector, there are $m!$ possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a *motif*. Let $\\pi_i^{m, \\tau}$ denote the motif associated with the $m$-dimensional state vector $\\mathbf{x_i}^{m, \\tau}$, and let $R$ be the number of distinct motifs that can be constructed from the $N$ state vectors. Then there are at most $R$ motifs; $R = N$ precisely when all motifs are unique, and $R = 1$ when all motifs are the same.\n",
       "\n",
       "Each unique motif $\\pi_i^{m, \\tau}$ can be mapped to a unique integer symbol $0 \\leq s_i \\leq M!-1$. Let $S(\\pi) : \\mathbb{R}^m \\to \\mathbb{N}_0$ be the function that maps the motif $\\pi$ to its symbol $s$, and let $\\Pi$ denote the set of symbols $\\Pi = \\{ s_i \\}_{i\\in \\{ 1, \\ldots, R\\}}$.\n",
       "\n",
       "### Probability computation\n",
       "\n",
       "#### `SymbolicPermutation`\n",
       "\n",
       "The probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from [^Fadlallah2013]),\n",
       "\n",
       "$$\n",
       "p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left(\\mathbf{x}_k^{m, \\tau} \\right) }{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi} \\left(\\mathbf{x}_k^{m, \\tau} \\right)} = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left(\\mathbf{x}_k^{m, \\tau} \\right) }{N},\n",
       "$$\n",
       "\n",
       "where the function $\\mathbf{1}_A(u)$ is the indicator function of a set $A$. That     is, $\\mathbf{1}_A(u) = 1$ if $u \\in A$, and $\\mathbf{1}_A(u) = 0$ otherwise.\n",
       "\n",
       "#### `SymbolicAmplitudeAwarePermutation`\n",
       "\n",
       "Amplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.\n",
       "\n",
       "$$\n",
       "p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, a_k}{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\,a_k} = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, a_k}{\\sum_{k=1}^N a_k}.\n",
       "$$\n",
       "\n",
       "The weights encoding amplitude information about state vector $\\mathbf{x}_i = (x_1^i, x_2^i, \\ldots, x_m^i)$ are\n",
       "\n",
       "$$\n",
       "a_i = \\dfrac{A}{m} \\sum_{k=1}^m |x_k^i | + \\dfrac{1-A}{d-1} \\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\n",
       "$$\n",
       "\n",
       "with $0 \\leq A \\leq 1$. When $A=0$ , only internal differences between the elements of $\\mathbf{x}_i$ are weighted. Only mean amplitude of the state vector elements are weighted when $A=1$. With, $0<A<1$, a combined weighting is used.\n",
       "\n",
       "#### `SymbolicWeightedPermutation`\n",
       "\n",
       "Weighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:\n",
       "\n",
       "$$\n",
       "p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i}\n",
       "\\left( \\mathbf{x}_k^{m, \\tau} \\right)\n",
       "\\, w_k}{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi}\n",
       "\\left( \\mathbf{x}_k^{m, \\tau} \\right) \\,w_k} = \\dfrac{\\sum_{k=1}^N\n",
       "\\mathbf{1}_{u:S(u) = s_i}\n",
       "\\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, w_k}{\\sum_{k=1}^N w_k}.\n",
       "$$\n",
       "\n",
       "The weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical ($w_j = \\beta \\,\\,\\, \\forall \\,\\,\\, j \\leq N$ and $\\beta > 0)$. Weights are dictated by the variance of the state vectors.\n",
       "\n",
       "Let the aritmetic mean of state vector $\\mathbf{x}_i$ be denoted by\n",
       "\n",
       "$$\n",
       "\\mathbf{\\hat{x}}_j^{m, \\tau} = \\frac{1}{m} \\sum_{k=1}^m x_{j + (k+1)\\tau}.\n",
       "$$\n",
       "\n",
       "Weights are then computed as\n",
       "\n",
       "$$\n",
       "w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j+(k+1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.\n",
       "$$\n",
       "\n",
       "*Note: in equation 7, section III, of the original paper, the authors write*\n",
       "\n",
       "$$\n",
       "w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j-(k-1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.\n",
       "$$\n",
       "\n",
       "*But given the formula they give for the arithmetic mean, this is **not** the variance of $\\mathbf{x}_i$, because the indices are mixed: $x_{j+(k-1)\\tau}$ in the weights formula, vs. $x_{j+(k+1)\\tau}$ in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for $\\mathbf{x}_i$*.\n",
       "\n",
       "### Entropy computation\n",
       "\n",
       "The generalized order-`α` Renyi entropy[^Rényi1960] can be computed over the probability distribution of symbols as $H(m, \\tau, \\alpha) = \\dfrac{\\alpha}{1-\\alpha} \\log \\left( \\sum_{j=1}^R p_j^\\alpha \\right)$. Permutation entropy, as described in Bandt and Pompe (2002), is just the limiting case as $α \\to1$, that is $H(m, \\tau) = - \\sum_j^R p(\\pi_j^{m, \\tau}) \\ln p(\\pi_j^{m, \\tau})$.\n",
       "\n",
       "*Note: Do not confuse the order of the generalized entropy (`α`) with the order `m` of the permutation entropy (which controls the symbol size). Permutation entropy is usually estimated with `α = 1`, but the implementation here allows the generalized entropy of any dimension to be computed from the symbol frequency distribution.*\n",
       "\n",
       "[^BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n",
       "\n",
       "[^Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n",
       "\n",
       "[^Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n",
       "\n",
       "[^Rényi1960]: A. Rényi, *Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability*, pp 547 (1960)\n",
       "\n",
       "[^Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n",
       "\n",
       "[^Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n",
       "\n",
       "[^Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n"
      ],
      "text/plain": [
       "\u001b[36m  SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\u001b[39m\n",
       "\u001b[36m  SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\u001b[39m\n",
       "\u001b[36m  SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\u001b[39m\n",
       "\n",
       "  Symbolic, permutation-based probabilities/entropy estimators.\n",
       "\n",
       "  Uses embedding dimension \u001b[35mm = 3\u001b[39m with embedding lag \u001b[35m\\tau = 1\u001b[39m by default. The\n",
       "  minimum dimension \u001b[35mm\u001b[39m is 2 (there are no sorting permutations of\n",
       "  single-element state vectors).\n",
       "\n",
       "\u001b[1m  Repeated values during symbolization\u001b[22m\n",
       "\u001b[1m  ======================================\u001b[22m\n",
       "\n",
       "  In the original implementation of permutation entropy \u001b[1m[^BandtPompe2002]\u001b[22m,\n",
       "  equal values are ordered after their order of appearance, but this can lead\n",
       "  to erroneous temporal correlations, especially for data with low-amplitude\n",
       "  resolution \u001b[1m[^Zunino2017]\u001b[22m. Here, we resolve this issue by letting the user\n",
       "  provide a custom \"less-than\" function. The keyword \u001b[36mlt\u001b[39m accepts a function\n",
       "  that decides which of two state vector elements are smaller. If two elements\n",
       "  are equal, the default behaviour is to randomly assign one of them as the\n",
       "  largest (\u001b[36mlt = Entropies.isless_rand\u001b[39m). For data with low amplitude\n",
       "  resolution, computing probabilities multiple times using the random approach\n",
       "  may reduce these erroneous effects.\n",
       "\n",
       "  To get the behaviour described in Bandt and Pompe (2002), use \u001b[36mlt =\n",
       "  Base.isless\u001b[39m).\n",
       "\n",
       "\u001b[1m  Properties of original signal preserved\u001b[22m\n",
       "\u001b[1m  =========================================\u001b[22m\n",
       "\n",
       "    •    \u001b[1m\u001b[36mSymbolicPermutation\u001b[39m\u001b[22m: Preserves ordinal patterns of state vectors\n",
       "        (sorting information). This implementation is based on Bandt &\n",
       "        Pompe et al. (2002)\u001b[1m[^BandtPompe2002]\u001b[22m and Berger et al. (2019)\n",
       "        \u001b[1m[^Berger2019]\u001b[22m.\n",
       "\n",
       "    •    \u001b[1m\u001b[36mSymbolicWeightedPermutation\u001b[39m\u001b[22m: Like \u001b[36mSymbolicPermutation\u001b[39m, but also\n",
       "        encodes amplitude information by tracking the variance of the\n",
       "        state vectors. This implementation is based on Fadlallah et al.\n",
       "        (2013)\u001b[1m[^Fadlallah2013]\u001b[22m.\n",
       "\n",
       "    •    \u001b[1m\u001b[36mSymbolicAmplitudeAwarePermutation\u001b[39m\u001b[22m: Like \u001b[36mSymbolicPermutation\u001b[39m, but\n",
       "        also encodes amplitude information by considering a weighted\n",
       "        combination of \u001b[4mabsolute amplitudes\u001b[24m of state vectors, and \u001b[4mrelative\n",
       "        differences between elements\u001b[24m of state vectors. See description\n",
       "        below for explanation of the weighting parameter \u001b[36mA\u001b[39m. This\n",
       "        implementation is based on Azami & Escudero (2016) \u001b[1m[^Azami2016]\u001b[22m.\n",
       "\n",
       "\u001b[1m  Probability estimation\u001b[22m\n",
       "\u001b[1m  ========================\u001b[22m\n",
       "\n",
       "\u001b[1m  Univariate time series\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  To estimate probabilities or entropies from univariate time series, use the\n",
       "  following methods:\n",
       "\n",
       "    •    \u001b[36mprobabilities(x::AbstractVector,\n",
       "        est::SymbolicProbabilityEstimator)\u001b[39m. Constructs state vectors from\n",
       "        \u001b[36mx\u001b[39m using embedding lag \u001b[36mτ\u001b[39m and embedding dimension \u001b[36mm\u001b[39m, symbolizes\n",
       "        state vectors, and computes probabilities as (weighted) relative\n",
       "        frequency of symbols.\n",
       "\n",
       "    •    \u001b[36mgenentropy(x::AbstractVector, est::SymbolicProbabilityEstimator;\n",
       "        α=1, base = 2)\u001b[39m computes probabilities by calling\n",
       "        \u001b[36mprobabilities(x::AbstractVector, est)\u001b[39m, then computer the order-\u001b[36mα\u001b[39m\n",
       "        generalized entropy to the given base.\n",
       "\n",
       "\u001b[1m  Speeding up repeated computations\u001b[22m\n",
       "\u001b[1m  -----------------------------------\u001b[22m\n",
       "\n",
       "  A pre-allocated integer symbol array \u001b[36ms\u001b[39m can be provided to save some memory\n",
       "  allocations if the probabilities are to be computed for multiple data sets.\n",
       "\n",
       "  \u001b[4mNote: it is not the array that will hold the final probabilities that is\n",
       "  pre-allocated, but the temporary integer array containing the symbolized\n",
       "  data points. Thus, if provided, it is required that \u001b[36mlength(x) == length(s)\u001b[39m\n",
       "  if \u001b[36mx\u001b[39m is a Dataset, or \u001b[36mlength(s) == length(x) - (m-1)τ\u001b[39m if \u001b[36mx\u001b[39m is a univariate\n",
       "  signal that is to be embedded first\u001b[24m.\n",
       "\n",
       "  Use the following signatures (only works for \u001b[36mSymbolicPermutation\u001b[39m).\n",
       "\n",
       "\u001b[36m  probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities\u001b[39m\n",
       "\u001b[36m  probabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities\u001b[39m\n",
       "\n",
       "\u001b[1m  Multivariate datasets\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Although not dealt with in the original paper describing the estimators,\n",
       "  numerically speaking, permutation entropies can also be computed for\n",
       "  multivariate datasets with dimension ≥ 2 (but see caveat below). Such\n",
       "  datasets may be, for example, preembedded time series. Then, just skip the\n",
       "  delay reconstruction step, compute and symbols directly from the \u001b[35mL\u001b[39m existing\n",
       "  state vectors \u001b[35m\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x_L}\\}\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mprobabilities(x::AbstractDataset,\n",
       "        est::SymbolicProbabilityEstimator)\u001b[39m. Compute ordinal patterns of\n",
       "        the state vectors of \u001b[36mx\u001b[39m directly (without doing any embedding),\n",
       "        symbolize those patterns, and compute probabilities as (weighted)\n",
       "        relative frequencies of symbols.\n",
       "\n",
       "    •    \u001b[36mgenentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator)\u001b[39m.\n",
       "        Computes probabilities from symbol frequencies using\n",
       "        \u001b[36mprobabilities(x::AbstractDataset,\n",
       "        est::SymbolicProbabilityEstimator)\u001b[39m, then computes the order-\u001b[36mα\u001b[39m\n",
       "        generalized (permutation) entropy to the given base.\n",
       "\n",
       "  \u001b[4mCaveat: A dynamical interpretation of the permutation entropy does not\n",
       "  necessarily hold if computing it on generic multivariate datasets. Method\n",
       "  signatures for \u001b[36mDataset\u001b[39ms are provided for convenience, and should only be\n",
       "  applied if you understand the relation between your input data, the\n",
       "  numerical value for the permutation entropy, and its interpretation.\u001b[24m\n",
       "\n",
       "\u001b[1m  Description\u001b[22m\n",
       "\u001b[1m  =============\u001b[22m\n",
       "\n",
       "  All symbolic estimators use the same underlying approach to estimating\n",
       "  probabilities.\n",
       "\n",
       "\u001b[1m  Embedding, ordinal patterns and symbolization\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Consider the \u001b[35mn\u001b[39m-element univariate time series \u001b[35m\\{x(t) = x_1, x_2, \\ldots,\n",
       "  x_n\\}\u001b[39m. Let \u001b[35m\\mathbf{x_i}^{m, \\tau} = \\{x_j, x_{j+\\tau}, \\ldots,\n",
       "  x_{j+(m-1)\\tau}\\}\u001b[39m for \u001b[35mj = 1, 2, \\ldots n - (m-1)\\tau\u001b[39m be the \u001b[35mi\u001b[39m-th state\n",
       "  vector in a delay reconstruction with embedding dimension \u001b[35mm\u001b[39m and\n",
       "  reconstruction lag \u001b[35m\\tau\u001b[39m. There are then \u001b[35mN = n - (m-1)\\tau\u001b[39m state vectors.\n",
       "\n",
       "  For an \u001b[35mm\u001b[39m-dimensional vector, there are \u001b[35mm!\u001b[39m possible ways of sorting it in\n",
       "  ascending order of magnitude. Each such possible sorting ordering is called\n",
       "  a \u001b[4mmotif\u001b[24m. Let \u001b[35m\\pi_i^{m, \\tau}\u001b[39m denote the motif associated with the\n",
       "  \u001b[35mm\u001b[39m-dimensional state vector \u001b[35m\\mathbf{x_i}^{m, \\tau}\u001b[39m, and let \u001b[35mR\u001b[39m be the number\n",
       "  of distinct motifs that can be constructed from the \u001b[35mN\u001b[39m state vectors. Then\n",
       "  there are at most \u001b[35mR\u001b[39m motifs; \u001b[35mR = N\u001b[39m precisely when all motifs are unique, and\n",
       "  \u001b[35mR = 1\u001b[39m when all motifs are the same.\n",
       "\n",
       "  Each unique motif \u001b[35m\\pi_i^{m, \\tau}\u001b[39m can be mapped to a unique integer symbol \u001b[35m0\n",
       "  \\leq s_i \\leq M!-1\u001b[39m. Let \u001b[35mS(\\pi) : \\mathbb{R}^m \\to \\mathbb{N}_0\u001b[39m be the\n",
       "  function that maps the motif \u001b[35m\\pi\u001b[39m to its symbol \u001b[35ms\u001b[39m, and let \u001b[35m\\Pi\u001b[39m denote the set\n",
       "  of symbols \u001b[35m\\Pi = \\{ s_i \\}_{i\\in \\{ 1, \\ldots, R\\}}\u001b[39m.\n",
       "\n",
       "\u001b[1m  Probability computation\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  \u001b[36mSymbolicPermutation\u001b[39m\u001b[22m\n",
       "\u001b[1m  ---------------------\u001b[22m\n",
       "\n",
       "  The probability of a given motif is its frequency of occurrence, normalized\n",
       "  by the total number of motifs (with notation from \u001b[1m[^Fadlallah2013]\u001b[22m),\n",
       "\n",
       "\u001b[35m  p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left(\\mathbf{x}_k^{m, \\tau} \\right) }{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi} \\left(\\mathbf{x}_k^{m, \\tau} \\right)} = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left(\\mathbf{x}_k^{m, \\tau} \\right) }{N},\u001b[39m\n",
       "\n",
       "  where the function \u001b[35m\\mathbf{1}_A(u)\u001b[39m is the indicator function of a set \u001b[35mA\u001b[39m.\n",
       "  That is, \u001b[35m\\mathbf{1}_A(u) = 1\u001b[39m if \u001b[35mu \\in A\u001b[39m, and \u001b[35m\\mathbf{1}_A(u) = 0\u001b[39m otherwise.\n",
       "\n",
       "\u001b[1m  \u001b[36mSymbolicAmplitudeAwarePermutation\u001b[39m\u001b[22m\n",
       "\u001b[1m  -----------------------------------\u001b[22m\n",
       "\n",
       "  Amplitude-aware permutation entropy is computed analogously to regular\n",
       "  permutation entropy but probabilities are weighted by amplitude information\n",
       "  as follows.\n",
       "\n",
       "\u001b[35m  p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, a_k}{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\,a_k} = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i} \\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, a_k}{\\sum_{k=1}^N a_k}.\u001b[39m\n",
       "\n",
       "  The weights encoding amplitude information about state vector \u001b[35m\\mathbf{x}_i =\n",
       "  (x_1^i, x_2^i, \\ldots, x_m^i)\u001b[39m are\n",
       "\n",
       "\u001b[35m  a_i = \\dfrac{A}{m} \\sum_{k=1}^m |x_k^i | + \\dfrac{1-A}{d-1} \\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\u001b[39m\n",
       "\n",
       "  with \u001b[35m0 \\leq A \\leq 1\u001b[39m. When \u001b[35mA=0\u001b[39m , only internal differences between the\n",
       "  elements of \u001b[35m\\mathbf{x}_i\u001b[39m are weighted. Only mean amplitude of the state\n",
       "  vector elements are weighted when \u001b[35mA=1\u001b[39m. With, \u001b[35m0<A<1\u001b[39m, a combined weighting is\n",
       "  used.\n",
       "\n",
       "\u001b[1m  \u001b[36mSymbolicWeightedPermutation\u001b[39m\u001b[22m\n",
       "\u001b[1m  -----------------------------\u001b[22m\n",
       "\n",
       "  Weighted permutation entropy is also computed analogously to regular\n",
       "  permutation entropy, but adds weights that encode amplitude information too:\n",
       "\n",
       "\u001b[35m  p(\\pi_i^{m, \\tau}) = \\dfrac{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) = s_i}\u001b[39m\n",
       "\u001b[35m\\left( \\mathbf{x}_k^{m, \\tau} \\right)\u001b[39m\n",
       "\u001b[35m\\, w_k}{\\sum_{k=1}^N \\mathbf{1}_{u:S(u) \\in \\Pi}\u001b[39m\n",
       "\u001b[35m\\left( \\mathbf{x}_k^{m, \\tau} \\right) \\,w_k} = \\dfrac{\\sum_{k=1}^N\u001b[39m\n",
       "\u001b[35m\\mathbf{1}_{u:S(u) = s_i}\u001b[39m\n",
       "\u001b[35m\\left( \\mathbf{x}_k^{m, \\tau} \\right) \\, w_k}{\\sum_{k=1}^N w_k}.\u001b[39m\n",
       "\n",
       "  The weighted permutation entropy is equivalent to regular permutation\n",
       "  entropy when weights are positive and identical (\u001b[35mw_j = \\beta \\,\\,\\, \\forall\n",
       "  \\,\\,\\, j \\leq N\u001b[39m and \u001b[35m\\beta > 0)\u001b[39m. Weights are dictated by the variance of the\n",
       "  state vectors.\n",
       "\n",
       "  Let the aritmetic mean of state vector \u001b[35m\\mathbf{x}_i\u001b[39m be denoted by\n",
       "\n",
       "\u001b[35m  \\mathbf{\\hat{x}}_j^{m, \\tau} = \\frac{1}{m} \\sum_{k=1}^m x_{j + (k+1)\\tau}.\u001b[39m\n",
       "\n",
       "  Weights are then computed as\n",
       "\n",
       "\u001b[35m  w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j+(k+1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.\u001b[39m\n",
       "\n",
       "  \u001b[4mNote: in equation 7, section III, of the original paper, the authors write\u001b[24m\n",
       "\n",
       "\u001b[35m  w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j-(k-1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.\u001b[39m\n",
       "\n",
       "  \u001b[4mBut given the formula they give for the arithmetic mean, this is \u001b[1mnot\u001b[22m the\n",
       "  variance of \u001b[35m\\mathbf{x}_i\u001b[39m, because the indices are mixed: \u001b[35mx_{j+(k-1)\\tau}\u001b[39m in\n",
       "  the weights formula, vs. \u001b[35mx_{j+(k+1)\\tau}\u001b[39m in the arithmetic mean formula.\n",
       "  This seems to imply that amplitude information about previous delay vectors\n",
       "  are mixed with mean amplitude information about current vectors. The authors\n",
       "  also mix the terms \"vector\" and \"neighboring vector\" (but uses the same\n",
       "  notation for both), making it hard to interpret whether the sign switch is a\n",
       "  typo or intended. Here, we use the notation above, which actually computes\n",
       "  the variance for \u001b[35m\\mathbf{x}_i\u001b[39m\u001b[24m.\n",
       "\n",
       "\u001b[1m  Entropy computation\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  The generalized order-\u001b[36mα\u001b[39m Renyi entropy\u001b[1m[^Rényi1960]\u001b[22m can be computed over the\n",
       "  probability distribution of symbols as \u001b[35mH(m, \\tau, \\alpha) =\n",
       "  \\dfrac{\\alpha}{1-\\alpha} \\log \\left( \\sum_{j=1}^R p_j^\\alpha \\right)\u001b[39m.\n",
       "  Permutation entropy, as described in Bandt and Pompe (2002), is just the\n",
       "  limiting case as \u001b[35mα \\to1\u001b[39m, that is \u001b[35mH(m, \\tau) = - \\sum_j^R p(\\pi_j^{m, \\tau})\n",
       "  \\ln p(\\pi_j^{m, \\tau})\u001b[39m.\n",
       "\n",
       "  \u001b[4mNote: Do not confuse the order of the generalized entropy (\u001b[36mα\u001b[39m) with the order\n",
       "  \u001b[36mm\u001b[39m of the permutation entropy (which controls the symbol size). Permutation\n",
       "  entropy is usually estimated with \u001b[36mα = 1\u001b[39m, but the implementation here allows\n",
       "  the generalized entropy of any dimension to be computed from the symbol\n",
       "  frequency distribution.\u001b[24m\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^BandtPompe2002]\u001b[22m\n",
       "  │\n",
       "  │  Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural\n",
       "  │  complexity measure for time series.\" Physical review letters 88.17\n",
       "  │  (2002): 174102.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Berger2019]\u001b[22m\n",
       "  │\n",
       "  │  Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a\n",
       "  │  Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\"\n",
       "  │  Entropy 21.10 (2019): 1023.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Fadlallah2013]\u001b[22m\n",
       "  │\n",
       "  │  Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A\n",
       "  │  complexity measure for time series incorporating amplitude\n",
       "  │  information.\" Physical Review E 87.2 (2013): 022911.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Rényi1960]\u001b[22m\n",
       "  │\n",
       "  │  A. Rényi, \u001b[4mProceedings of the fourth Berkeley Symposium on\n",
       "  │  Mathematics, Statistics and Probability\u001b[24m, pp 547 (1960)\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Azami2016]\u001b[22m\n",
       "  │\n",
       "  │  Azami, H., & Escudero, J. (2016). Amplitude-aware permutation\n",
       "  │  entropy: Illustration in spike detection and signal segmentation.\n",
       "  │  Computer methods and programs in biomedicine, 128, 40-51.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Fadlallah2013]\u001b[22m\n",
       "  │\n",
       "  │  Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A\n",
       "  │  complexity measure for time series incorporating amplitude\n",
       "  │  information.\" Physical Review E 87.2 (2013): 022911.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Zunino2017]\u001b[22m\n",
       "  │\n",
       "  │  Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017).\n",
       "  │  Permutation entropy based time series analysis: Equalities in the\n",
       "  │  input signal can lead to false conclusions. Physics Letters A,\n",
       "  │  381(22), 1883-1892."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?SymbolicPermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "genentropy(p::Probabilities; α = 1.0, base = Base.MathConstants.e)\n",
       "\\end{verbatim}\n",
       "Compute the generalized order-\\texttt{α} entropy of some probabilities returned by the \\href{@ref}{\\texttt{probabilities}} function. Alternatively, compute entropy from pre-computed \\texttt{Probabilities}.\n",
       "\n",
       "\\subsection{Description}\n",
       "Let $p$ be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n",
       "\n",
       "$$H_\\alpha(p) = \\frac{1}{1-\\alpha} \\log \\left(\\sum_i p[i]^\\alpha\\right)$$\n",
       "and generalizes other known entropies, like e.g. the information entropy ($\\alpha = 1$, see \\footnotemark[Shannon1948]), the maximum entropy ($\\alpha=0$, also known as Hartley entropy), or the correlation entropy ($\\alpha = 2$, also known as collision entropy).\n",
       "\n",
       "\\begin{verbatim}\n",
       "genentropy(x::Vector_or_Dataset, est; α = 1.0, base)\n",
       "\\end{verbatim}\n",
       "A convenience syntax, which calls first \\texttt{probabilities(x, est)} and then calculates the entropy of the result (and thus \\texttt{est} can be a \\texttt{ProbabilitiesEstimator} or simply \\texttt{ε::Real}).\n",
       "\n",
       "\\footnotetext[Rényi1960]{A. Rényi, \\emph{Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability}, pp 547 (1960)\n",
       "\n",
       "}\n",
       "\\footnotetext[Shannon1948]{C. E. Shannon, Bell Systems Technical Journal \\textbf{27}, pp 379 (1948)\n",
       "\n",
       "}\n"
      ],
      "text/markdown": [
       "```\n",
       "genentropy(p::Probabilities; α = 1.0, base = Base.MathConstants.e)\n",
       "```\n",
       "\n",
       "Compute the generalized order-`α` entropy of some probabilities returned by the [`probabilities`](@ref) function. Alternatively, compute entropy from pre-computed `Probabilities`.\n",
       "\n",
       "## Description\n",
       "\n",
       "Let $p$ be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n",
       "\n",
       "$$\n",
       "H_\\alpha(p) = \\frac{1}{1-\\alpha} \\log \\left(\\sum_i p[i]^\\alpha\\right)\n",
       "$$\n",
       "\n",
       "and generalizes other known entropies, like e.g. the information entropy ($\\alpha = 1$, see [^Shannon1948]), the maximum entropy ($\\alpha=0$, also known as Hartley entropy), or the correlation entropy ($\\alpha = 2$, also known as collision entropy).\n",
       "\n",
       "```\n",
       "genentropy(x::Vector_or_Dataset, est; α = 1.0, base)\n",
       "```\n",
       "\n",
       "A convenience syntax, which calls first `probabilities(x, est)` and then calculates the entropy of the result (and thus `est` can be a `ProbabilitiesEstimator` or simply `ε::Real`).\n",
       "\n",
       "[^Rényi1960]: A. Rényi, *Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability*, pp 547 (1960)\n",
       "\n",
       "[^Shannon1948]: C. E. Shannon, Bell Systems Technical Journal **27**, pp 379 (1948)\n"
      ],
      "text/plain": [
       "\u001b[36m  genentropy(p::Probabilities; α = 1.0, base = Base.MathConstants.e)\u001b[39m\n",
       "\n",
       "  Compute the generalized order-\u001b[36mα\u001b[39m entropy of some probabilities returned by\n",
       "  the \u001b[36mprobabilities\u001b[39m function. Alternatively, compute entropy from pre-computed\n",
       "  \u001b[36mProbabilities\u001b[39m.\n",
       "\n",
       "\u001b[1m  Description\u001b[22m\n",
       "\u001b[1m  =============\u001b[22m\n",
       "\n",
       "  Let \u001b[35mp\u001b[39m be an array of probabilities (summing to 1). Then the generalized\n",
       "  (Rényi) entropy is\n",
       "\n",
       "\u001b[35m  H_\\alpha(p) = \\frac{1}{1-\\alpha} \\log \\left(\\sum_i p[i]^\\alpha\\right)\u001b[39m\n",
       "\n",
       "  and generalizes other known entropies, like e.g. the information entropy\n",
       "  (\u001b[35m\\alpha = 1\u001b[39m, see \u001b[1m[^Shannon1948]\u001b[22m), the maximum entropy (\u001b[35m\\alpha=0\u001b[39m, also known\n",
       "  as Hartley entropy), or the correlation entropy (\u001b[35m\\alpha = 2\u001b[39m, also known as\n",
       "  collision entropy).\n",
       "\n",
       "\u001b[36m  genentropy(x::Vector_or_Dataset, est; α = 1.0, base)\u001b[39m\n",
       "\n",
       "  A convenience syntax, which calls first \u001b[36mprobabilities(x, est)\u001b[39m and then\n",
       "  calculates the entropy of the result (and thus \u001b[36mest\u001b[39m can be a\n",
       "  \u001b[36mProbabilitiesEstimator\u001b[39m or simply \u001b[36mε::Real\u001b[39m).\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Rényi1960]\u001b[22m\n",
       "  │\n",
       "  │  A. Rényi, \u001b[4mProceedings of the fourth Berkeley Symposium on\n",
       "  │  Mathematics, Statistics and Probability\u001b[24m, pp 547 (1960)\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Shannon1948]\u001b[22m\n",
       "  │\n",
       "  │  C. E. Shannon, Bell Systems Technical Journal \u001b[1m27\u001b[22m, pp 379 (1948)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Entropies.genentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111-element Array{Float64,1}:\n",
       " 0.5013315584795768\n",
       " 0.12451044461396998\n",
       " 0.16258231055262984\n",
       " 0.306055701842872\n",
       " 0.4720676863964215\n",
       " 0.9719158653642606\n",
       " 0.9048591121920107\n",
       " 0.21611412114831818\n",
       " 0.8186677441546872\n",
       " 0.6772617109643466\n",
       " 0.6936713690092906\n",
       " 0.09275378402303369\n",
       " 0.5239216319899276\n",
       " ⋮\n",
       " 0.9374026231941286\n",
       " 0.3230773894694914\n",
       " 0.16163669707629058\n",
       " 0.7387528787285356\n",
       " 0.09880989537082518\n",
       " 0.11624217768172485\n",
       " 0.6790420214560267\n",
       " 0.9187275332454685\n",
       " 0.6877447486874149\n",
       " 0.6124941430015418\n",
       " 0.9446167717501188\n",
       " 0.38805991306667575"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rand(111)\n",
    "y = rand(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "genembed(s, τs, js = ones(...); ws = nothing) → dataset\n",
       "\\end{verbatim}\n",
       "Create a generalized embedding of \\texttt{s} which can be a timeseries or arbitrary \\texttt{Dataset}, and return the result as a new \\texttt{Dataset}.\n",
       "\n",
       "The generalized embedding works as follows:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{τs} denotes what delay times will be used for each of the entries of the delay vector. It is recommended that \\texttt{τs[1] = 0}. \\texttt{τs} is allowed to have \\emph{negative entries} as well.\n",
       "\n",
       "\n",
       "\\item \\texttt{js} denotes which of the timeseries contained in \\texttt{s} will be used for the entries of the delay vector. \\texttt{js} can contain duplicate indices.\n",
       "\n",
       "\n",
       "\\item \\texttt{ws} are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by \\texttt{ws[i]}). If provided, it is recommended that \\texttt{ws[1] = 1}\n",
       "\n",
       "\\end{itemize}\n",
       "\\texttt{τs, js, ws} are tuples (or vectors) of length \\texttt{D}, which also coincides with the embedding dimension. For example, imagine input trajectory $s = [x, y, z]$ where $x, y, z$ are timeseries (the columns of the \\texttt{Dataset}). If \\texttt{js = (1, 3, 2)} and \\texttt{τs = (0, 2, -7)} the created delay vector at each step $n$ will be\n",
       "\n",
       "$$(x(n), z(n+2), y(n-7))$$\n",
       "Using \\texttt{ws = (1, 0.5, 0.25)} as well would create\n",
       "\n",
       "$$(x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))$$\n",
       "\\texttt{js} can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if \\texttt{s} is a timeseries instead of a \\texttt{Dataset}.\n",
       "\n",
       "See also \\href{@ref}{\\texttt{embed}}. Internally uses \\href{@ref}{\\texttt{GeneralizedEmbedding}}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "genembed(s, τs, js = ones(...); ws = nothing) → dataset\n",
       "```\n",
       "\n",
       "Create a generalized embedding of `s` which can be a timeseries or arbitrary `Dataset`, and return the result as a new `Dataset`.\n",
       "\n",
       "The generalized embedding works as follows:\n",
       "\n",
       "  * `τs` denotes what delay times will be used for each of the entries of the delay vector. It is recommended that `τs[1] = 0`. `τs` is allowed to have *negative entries* as well.\n",
       "  * `js` denotes which of the timeseries contained in `s` will be used for the entries of the delay vector. `js` can contain duplicate indices.\n",
       "  * `ws` are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by `ws[i]`). If provided, it is recommended that `ws[1] = 1`\n",
       "\n",
       "`τs, js, ws` are tuples (or vectors) of length `D`, which also coincides with the embedding dimension. For example, imagine input trajectory $s = [x, y, z]$ where $x, y, z$ are timeseries (the columns of the `Dataset`). If `js = (1, 3, 2)` and `τs = (0, 2, -7)` the created delay vector at each step $n$ will be\n",
       "\n",
       "$$\n",
       "(x(n), z(n+2), y(n-7))\n",
       "$$\n",
       "\n",
       "Using `ws = (1, 0.5, 0.25)` as well would create\n",
       "\n",
       "$$\n",
       "(x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))\n",
       "$$\n",
       "\n",
       "`js` can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if `s` is a timeseries instead of a `Dataset`.\n",
       "\n",
       "See also [`embed`](@ref). Internally uses [`GeneralizedEmbedding`](@ref).\n"
      ],
      "text/plain": [
       "\u001b[36m  genembed(s, τs, js = ones(...); ws = nothing) → dataset\u001b[39m\n",
       "\n",
       "  Create a generalized embedding of \u001b[36ms\u001b[39m which can be a timeseries or arbitrary\n",
       "  \u001b[36mDataset\u001b[39m, and return the result as a new \u001b[36mDataset\u001b[39m.\n",
       "\n",
       "  The generalized embedding works as follows:\n",
       "\n",
       "    •    \u001b[36mτs\u001b[39m denotes what delay times will be used for each of the entries\n",
       "        of the delay vector. It is recommended that \u001b[36mτs[1] = 0\u001b[39m. \u001b[36mτs\u001b[39m is\n",
       "        allowed to have \u001b[4mnegative entries\u001b[24m as well.\n",
       "\n",
       "    •    \u001b[36mjs\u001b[39m denotes which of the timeseries contained in \u001b[36ms\u001b[39m will be used for\n",
       "        the entries of the delay vector. \u001b[36mjs\u001b[39m can contain duplicate indices.\n",
       "\n",
       "    •    \u001b[36mws\u001b[39m are optional weights that weight each embedded entry (the i-th\n",
       "        entry of the delay vector is weighted by \u001b[36mws[i]\u001b[39m). If provided, it\n",
       "        is recommended that \u001b[36mws[1] = 1\u001b[39m\n",
       "\n",
       "  \u001b[36mτs, js, ws\u001b[39m are tuples (or vectors) of length \u001b[36mD\u001b[39m, which also coincides with\n",
       "  the embedding dimension. For example, imagine input trajectory \u001b[35ms = [x, y, z]\u001b[39m\n",
       "  where \u001b[35mx, y, z\u001b[39m are timeseries (the columns of the \u001b[36mDataset\u001b[39m). If \u001b[36mjs = (1, 3, 2)\u001b[39m\n",
       "  and \u001b[36mτs = (0, 2, -7)\u001b[39m the created delay vector at each step \u001b[35mn\u001b[39m will be\n",
       "\n",
       "\u001b[35m  (x(n), z(n+2), y(n-7))\u001b[39m\n",
       "\n",
       "  Using \u001b[36mws = (1, 0.5, 0.25)\u001b[39m as well would create\n",
       "\n",
       "\u001b[35m  (x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))\u001b[39m\n",
       "\n",
       "  \u001b[36mjs\u001b[39m can be skipped, defaulting to index 1 (first timeseries) for all delay\n",
       "  entries, while it has no effect if \u001b[36ms\u001b[39m is a timeseries instead of a \u001b[36mDataset\u001b[39m.\n",
       "\n",
       "  See also \u001b[36membed\u001b[39m. Internally uses \u001b[36mGeneralizedEmbedding\u001b[39m."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DelayEmbeddings.genembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TE_{x \\to y} = \\sum p(x_t, y_t, y_{t+\\eta}) \\log \\dfrac{p(y_{t+\\eta} | y_t, x_t)}{p(y_{t+\\eta} | y_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-dimensional Dataset{Float64} with 110 points\n",
       " 0.307776    0.501332   0.12451\n",
       " 0.41414     0.12451    0.162582\n",
       " 0.678878    0.162582   0.306056\n",
       " 0.112134    0.306056   0.472068\n",
       " 0.111605    0.472068   0.971916\n",
       " 0.00754226  0.971916   0.904859\n",
       " 0.0713633   0.904859   0.216114\n",
       " 0.445997    0.216114   0.818668\n",
       " 0.469511    0.818668   0.677262\n",
       " 0.560266    0.677262   0.693671\n",
       " 0.62413     0.693671   0.0927538\n",
       " 0.896199    0.0927538  0.523922\n",
       " 0.350667    0.523922   0.190396\n",
       " ⋮                      \n",
       " 0.313168    0.339282   0.937403\n",
       " 0.934156    0.937403   0.323077\n",
       " 0.117355    0.323077   0.161637\n",
       " 0.840459    0.161637   0.738753\n",
       " 0.633377    0.738753   0.0988099\n",
       " 0.957893    0.0988099  0.116242\n",
       " 0.836779    0.116242   0.679042\n",
       " 0.20233     0.679042   0.918728\n",
       " 0.952093    0.918728   0.687745\n",
       " 0.521576    0.687745   0.612494\n",
       " 0.684139    0.612494   0.944617\n",
       " 0.427641    0.944617   0.38806"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint= DelayEmbeddings.genembed(Dataset(x, y),  (0,0,1), (1,2,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n",
       "\\end{verbatim}\n",
       "Compute the generalized order-\\texttt{q} entropy of some probabilities returned by the \\href{@ref}{\\texttt{probabilities}} function. Alternatively, compute entropy from pre-computed \\texttt{Probabilities}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "genentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n",
       "\\end{verbatim}\n",
       "A convenience syntax, which calls first \\texttt{probabilities(x, est)} and then calculates the entropy of the result (and thus \\texttt{est} can be a \\texttt{ProbabilitiesEstimator} or simply \\texttt{ε::Real}).\n",
       "\n",
       "\\subsection{Description}\n",
       "Let $p$ be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n",
       "\n",
       "$$H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)$$\n",
       "and generalizes other known entropies, like e.g. the information entropy ($q = 1$, see \\footnotemark[Shannon1948]), the maximum entropy ($q=0$, also known as Hartley entropy), or the correlation entropy ($q = 2$, also known as collision entropy).\n",
       "\n",
       "\\footnotetext[Rényi1960]{A. Rényi, \\emph{Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability}, pp 547 (1960)\n",
       "\n",
       "}\n",
       "\\footnotetext[Shannon1948]{C. E. Shannon, Bell Systems Technical Journal \\textbf{27}, pp 379 (1948)\n",
       "\n",
       "}\n"
      ],
      "text/markdown": [
       "```\n",
       "genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n",
       "```\n",
       "\n",
       "Compute the generalized order-`q` entropy of some probabilities returned by the [`probabilities`](@ref) function. Alternatively, compute entropy from pre-computed `Probabilities`.\n",
       "\n",
       "```\n",
       "genentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n",
       "```\n",
       "\n",
       "A convenience syntax, which calls first `probabilities(x, est)` and then calculates the entropy of the result (and thus `est` can be a `ProbabilitiesEstimator` or simply `ε::Real`).\n",
       "\n",
       "## Description\n",
       "\n",
       "Let $p$ be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n",
       "\n",
       "$$\n",
       "H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)\n",
       "$$\n",
       "\n",
       "and generalizes other known entropies, like e.g. the information entropy ($q = 1$, see [^Shannon1948]), the maximum entropy ($q=0$, also known as Hartley entropy), or the correlation entropy ($q = 2$, also known as collision entropy).\n",
       "\n",
       "[^Rényi1960]: A. Rényi, *Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability*, pp 547 (1960)\n",
       "\n",
       "[^Shannon1948]: C. E. Shannon, Bell Systems Technical Journal **27**, pp 379 (1948)\n"
      ],
      "text/plain": [
       "\u001b[36m  genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\u001b[39m\n",
       "\n",
       "  Compute the generalized order-\u001b[36mq\u001b[39m entropy of some probabilities returned by\n",
       "  the \u001b[36mprobabilities\u001b[39m function. Alternatively, compute entropy from pre-computed\n",
       "  \u001b[36mProbabilities\u001b[39m.\n",
       "\n",
       "\u001b[36m  genentropy(x::Vector_or_Dataset, est; q = 1.0, base)\u001b[39m\n",
       "\n",
       "  A convenience syntax, which calls first \u001b[36mprobabilities(x, est)\u001b[39m and then\n",
       "  calculates the entropy of the result (and thus \u001b[36mest\u001b[39m can be a\n",
       "  \u001b[36mProbabilitiesEstimator\u001b[39m or simply \u001b[36mε::Real\u001b[39m).\n",
       "\n",
       "\u001b[1m  Description\u001b[22m\n",
       "\u001b[1m  =============\u001b[22m\n",
       "\n",
       "  Let \u001b[35mp\u001b[39m be an array of probabilities (summing to 1). Then the generalized\n",
       "  (Rényi) entropy is\n",
       "\n",
       "\u001b[35m  H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)\u001b[39m\n",
       "\n",
       "  and generalizes other known entropies, like e.g. the information entropy (\u001b[35mq\n",
       "  = 1\u001b[39m, see \u001b[1m[^Shannon1948]\u001b[22m), the maximum entropy (\u001b[35mq=0\u001b[39m, also known as Hartley\n",
       "  entropy), or the correlation entropy (\u001b[35mq = 2\u001b[39m, also known as collision\n",
       "  entropy).\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Rényi1960]\u001b[22m\n",
       "  │\n",
       "  │  A. Rényi, \u001b[4mProceedings of the fourth Berkeley Symposium on\n",
       "  │  Mathematics, Statistics and Probability\u001b[24m, pp 547 (1960)\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Shannon1948]\u001b[22m\n",
       "  │\n",
       "  │  C. E. Shannon, Bell Systems Technical Journal \u001b[1m27\u001b[22m, pp 379 (1948)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Entropies.genentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TE_{x \\to y} = - H(x_y, y_t, y_{t+\\eta}) + H(y_{t+\\eta}, y_t) +  H(x_y, y_t)  - H(y_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: RectangularBinning not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: RectangularBinning not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[7]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "b = Entropies.RectangularBinning(50)\n",
    "H3 = Entropies.genentropy(joint, KozachenkoLeonenko(1,10))# α =1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2a =Entropies.genentropy(joint[:,[3,2]], VisitationFrequency(b), α =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.675275013772055"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2b=Entropies.genentropy(joint[:,[1,2]], VisitationFrequency(b), α =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6766536997888504"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1=Entropies.genentropy(joint[:,[2]], VisitationFrequency(b), α =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986213139832043"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-H3+H2a+H2b-H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mV\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mq\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1my\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n",
       "\\end{verbatim}\n",
       "A probability estimator based on binning data into rectangular boxes dictated by the binning scheme \\texttt{r}.\n",
       "\n",
       "\\subsection{Example}\n",
       "\\begin{verbatim}\n",
       "# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\n",
       "b = RectangularBinning(5)\n",
       "\n",
       "# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n",
       "# over the boxes of the binning, constructed as describedon the previous line.\n",
       "est = VisitationFrequency(b)\n",
       "\\end{verbatim}\n",
       "See also: \\href{@ref}{\\texttt{RectangularBinning}}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n",
       "```\n",
       "\n",
       "A probability estimator based on binning data into rectangular boxes dictated by the binning scheme `r`.\n",
       "\n",
       "## Example\n",
       "\n",
       "```julia\n",
       "# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\n",
       "b = RectangularBinning(5)\n",
       "\n",
       "# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n",
       "# over the boxes of the binning, constructed as describedon the previous line.\n",
       "est = VisitationFrequency(b)\n",
       "```\n",
       "\n",
       "See also: [`RectangularBinning`](@ref).\n"
      ],
      "text/plain": [
       "\u001b[36m  VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\u001b[39m\n",
       "\n",
       "  A probability estimator based on binning data into rectangular boxes\n",
       "  dictated by the binning scheme \u001b[36mr\u001b[39m.\n",
       "\n",
       "\u001b[1m  Example\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "\u001b[36m  # Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\u001b[39m\n",
       "\u001b[36m  b = RectangularBinning(5)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # A probabilities estimator that, when applied a dataset, computes visitation frequencies\u001b[39m\n",
       "\u001b[36m  # over the boxes of the binning, constructed as describedon the previous line.\u001b[39m\n",
       "\u001b[36m  est = VisitationFrequency(b)\u001b[39m\n",
       "\n",
       "  See also: \u001b[36mRectangularBinning\u001b[39m."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?VisitationFrequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6655206093119226"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = RectangularBinning(6)\n",
    "H3 = Entropies.genentropy(joint, VisitationFrequency(b), α =1.0)\n",
    "H2a =Entropies.genentropy(joint[:,[3,2]], VisitationFrequency(b), α =1.0)\n",
    "H2b=Entropies.genentropy(joint[:,[1,2]], VisitationFrequency(b), α =1.0)\n",
    "H1=Entropies.genentropy(joint[:,[2]], VisitationFrequency(b), α =1.0)\n",
    "T =-H3+H2a+H2b-H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = RectangularBinning(6)\n",
    "H3 = Entropies.genentropy(joint, VisitationFrequency(b), α =1.0)\n",
    "H2a =Entropies.genentropy(joint[:,[3,2]], VisitationFrequency(b), α =1.0)\n",
    "H2b=Entropies.genentropy(joint[:,[1,2]], VisitationFrequency(b), α =1.0)\n",
    "H1=Entropies.genentropy(joint[:,[2]], VisitationFrequency(b), α =1.0)\n",
    "T =-H3+H2a+H2b-H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\subsection{k-th nearest neighbour(kNN) based}\n",
       "\\begin{verbatim}\n",
       "Kraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n",
       "\\end{verbatim}\n",
       "Entropy estimator based on \\texttt{k}-th nearest neighbor searches\\footnotemark[Kraskov2004].\n",
       "\n",
       "\\texttt{w} is the number of nearest neighbors to exclude when searching for neighbours  (defaults to \\texttt{0}, meaning that only the point itself is excluded).\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{info}\n",
       "\n",
       "Info\n",
       "\n",
       "This estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n",
       "\n",
       "\\end{quote}\n",
       "\\footnotetext[Kraskov2004]{Kraskov, A., Stögbauer, H., \\& Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n",
       "\n",
       "}\n"
      ],
      "text/markdown": [
       "## k-th nearest neighbour(kNN) based\n",
       "\n",
       "```\n",
       "Kraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n",
       "```\n",
       "\n",
       "Entropy estimator based on `k`-th nearest neighbor searches[^Kraskov2004].\n",
       "\n",
       "`w` is the number of nearest neighbors to exclude when searching for neighbours  (defaults to `0`, meaning that only the point itself is excluded).\n",
       "\n",
       "!!! info\n",
       "    This estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n",
       "\n",
       "\n",
       "[^Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n"
      ],
      "text/plain": [
       "\u001b[1m  k-th nearest neighbour(kNN) based\u001b[22m\n",
       "\u001b[1m  ===================================\u001b[22m\n",
       "\n",
       "\u001b[36m  Kraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\u001b[39m\n",
       "\n",
       "  Entropy estimator based on \u001b[36mk\u001b[39m-th nearest neighbor searches\u001b[1m[^Kraskov2004]\u001b[22m.\n",
       "\n",
       "  \u001b[36mw\u001b[39m is the number of nearest neighbors to exclude when searching for\n",
       "  neighbours (defaults to \u001b[36m0\u001b[39m, meaning that only the point itself is excluded).\n",
       "\n",
       "\u001b[36m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  This estimator is only available for entropy estimation.\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  Probabilities cannot be obtained directly.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Kraskov2004]\u001b[22m\n",
       "  │\n",
       "  │  Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating\n",
       "  │  mutual information. Physical review E, 69(6), 066138."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Entropies.Kraskov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching genentropy(::Dataset{3,Float64}, ::Kraskov; α=1.0)\nClosest candidates are:\n  genentropy(::Dataset{D,T}, ::Kraskov; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/Kraskov.jl:29 got unsupported keyword argument \"α\"\n  genentropy(::Union{AbstractArray{T,1} where T, Dataset}, ::Any; α, base) at /Users/andreu/.julia/packages/Entropies/Y329Q/src/core.jl:138\n  genentropy(::Dataset{D,T}, !Matched::KozachenkoLeonenko; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/KozachenkoLeonenko.jl:31 got unsupported keyword argument \"α\"",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching genentropy(::Dataset{3,Float64}, ::Kraskov; α=1.0)\nClosest candidates are:\n  genentropy(::Dataset{D,T}, ::Kraskov; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/Kraskov.jl:29 got unsupported keyword argument \"α\"\n  genentropy(::Union{AbstractArray{T,1} where T, Dataset}, ::Any; α, base) at /Users/andreu/.julia/packages/Entropies/Y329Q/src/core.jl:138\n  genentropy(::Dataset{D,T}, !Matched::KozachenkoLeonenko; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/KozachenkoLeonenko.jl:31 got unsupported keyword argument \"α\"",
      "",
      "Stacktrace:",
      " [1] kwerr(::NamedTuple{(:α,),Tuple{Float64}}, ::Function, ::Dataset{3,Float64}, ::Kraskov) at ./error.jl:157",
      " [2] top-level scope at In[39]:2",
      " [3] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "b =2\n",
    "H3 = Entropies.genentropy(joint, Kraskov(k=b), α =1.0)\n",
    "H2a =Entropies.genentropy(joint[:,[3,2]], Kraskov(k=b), α =1.0)\n",
    "H2b=Entropies.genentropy(joint[:,[1,2]], Kraskov(k=b), α =1.0)\n",
    "H1=Entropies.genentropy(joint[:,[2]], Kraskov(k=b), α =1.0)\n",
    "T =-H3+H2a+H2b-H1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
